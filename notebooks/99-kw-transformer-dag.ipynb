{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import paths\n",
    "from src.utils import save_json, load_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp  = paths['catalog_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset DAG Spec\n",
    "\n",
    "A transformer function takes in `input_datasets` and produces `output_datasets`.\n",
    "Edges can be thought of as directed, indicating a dependency. e.g. `output_datasets` depend on `input_datasets`\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"hyperedge_1\": {\n",
    "        \"input_datasets\":[],\n",
    "        \"output_datasets\":[],\n",
    "        transformations: [\n",
    "            (function_1, kwargs_dict_1 ),\n",
    "            (function_2, kwargs_dict_2 ),\n",
    "            ...\n",
    "        ],\n",
    "        \"suppress_output\": False,  # defaults to True\n",
    "    },\n",
    "    \"sink_edge\": {\n",
    "        \"datasource_name\": \"ds_name\",\n",
    "        \"datasource_opts\": {},\n",
    "        \"output_dataset: \"ds_name\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "* Are Sink edges special?\n",
    "  Right now they define a 1-1 map between a datasource and a dataset. (i.e. these are not hyperedges).\n",
    "\n",
    "* How to the list of transformers work with many in many out. Take/emit a dict of all of them?\n",
    "\n",
    "* How to implement traversal. (Apply_transforms should take a dataset name - optionally, and give me an ordered list of what to run)\n",
    "\n",
    "* add force flag to add_transformer, in case you are overwriting a key\n",
    "\n",
    "* should it be input_dataset or input_datasets? The most common use case is probably the former\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_list(str_or_iterable):\n",
    "    \"\"\"Convert strings to lists. Pass lists (or None) unchanged.\n",
    "    \"\"\"\n",
    "    if isinstance(str_or_iterable, str):\n",
    "        return [str_or_iterable]\n",
    "    if str_or_iterable is None:\n",
    "        return []\n",
    "    return str_or_iterable\n",
    "\n",
    "def get_transformer_dag(\n",
    "        dag_path=None,\n",
    "        dag_file=None,\n",
    "        include_filename=False,\n",
    "    ):\n",
    "    \"\"\"Get the list of transformation pipelines\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    If include_filename is True:\n",
    "        A tuple: (transformer_dag, dag_file_fq)\n",
    "    else:\n",
    "        transformer_dag\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_filename: boolean\n",
    "        if True, returns a tuple: (list, filename)\n",
    "    dag_path: path. (default: MODULE_DIR/data)\n",
    "        Location of `dag_file`\n",
    "    dag_file: string, default 'transformer_dag.json'\n",
    "        Name of json file that contains the transformer pipeline\n",
    "    \"\"\"\n",
    "    if dag_path is None:\n",
    "        dag_path = paths['catalog_path']\n",
    "    else:\n",
    "        dag_path = pathlib.Path(dag_path)\n",
    "    if dag_file is None:\n",
    "        dag_file = 'transformer_dag.json'\n",
    "\n",
    "    dag_file_fq = dag_path / dag_file\n",
    "    try:\n",
    "        transformer_dag = load_json(dag_file_fq)\n",
    "    except FileNotFoundError:\n",
    "        transformer_dag = {}\n",
    "        \n",
    "    if not isinstance(transformer_dag, dict):\n",
    "        raise Exception(\"Obsolete file format: transformer_dag must be a dict\")\n",
    "\n",
    "    if include_filename:\n",
    "        return transformer_dag, dag_file_fq\n",
    "    return transformer_dag\n",
    "\n",
    "\n",
    "def add_transformer(\n",
    "    name=None,\n",
    "    datasource_name=None,\n",
    "    datasource_opts=None,\n",
    "    input_datasets=None,\n",
    "    suppress_output=False,\n",
    "    output_datasets=None,\n",
    "    transformations=None,\n",
    "    dag_path=None,\n",
    "    dag_file=None,\n",
    "    write_to_catalog=True,\n",
    "    ):\n",
    "    \"\"\"Create and add a dataset transformation pipeline to the workflow.\n",
    "\n",
    "    Transformer pipelines apply a sequence of transformer functions to a Dataset (or DataSource),\n",
    "    to produce new Dataset objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Name for this transformer instance (must be unique).\n",
    "        By default, one will be created from the input and output dataset names; e.g.\n",
    "        _input_ds1_input_ds2_to_output_ds1\n",
    "    input_datasets: string or iterable\n",
    "        Upstream data dependencies. These must be present\n",
    "    output_datasets: string or Iterable\n",
    "        These datasets will be generated\n",
    "    datasource_name: string\n",
    "        Name of a DataSource to use to generate the output\n",
    "        Setting this option will create a source node in the dataset flow graph\n",
    "        (or a sink node in the data dependency graph).\n",
    "        Transformers of this type must specify at most one entry in `output_datasets`\n",
    "    datasource_opts: dict\n",
    "        Options to use when generating a Dataset from this DataSource\n",
    "    suppress_output: boolean\n",
    "        If True, the terminal dataset object is not written to disk.\n",
    "        This is useful when one of the intervening tranformers handles the writing; e.g. train/test split.\n",
    "    transformations: list of tuples\n",
    "        Squence of transformer functions to apply. tuples consist of:\n",
    "        (transformer_name, transformer_opts)\n",
    "    dag_path: path. (default: paths['catalog_path'])\n",
    "        Location of `dag_file`\n",
    "    dag_file: string, default 'transformer_dag.json'\n",
    "        Name of json file that contains the transformer pipeline\n",
    "    write_to_catalog: Boolean, Default True\n",
    "        If False, don't actually write this entry to the catalog.\n",
    "    Examples\n",
    "    --------\n",
    "    \n",
    "    If you only have one input or output, it may be specified simply as a string;\n",
    "    i.e. these are identical\n",
    "    >>> add_transformer(input_datasets='other', output_datasets='p_other', write_to_catalog=False)\n",
    "    {'_p_other': {'input_datasets': ['other'], 'output_datasets': ['p_other']}}\n",
    "    >>> add_transformer(input_datasets=['other'], output_datasets='p_other', write_to_catalog=False)\n",
    "    {'_p_other': {'input_datasets': ['other'], 'output_datasets': ['p_other']}}\n",
    "    \n",
    "    >>> add_transformer(input_datasets=['cc-by', 'cc-by-nc'], output_datasets='cc', write_to_catalog=False)\n",
    "    {'_cc': {'input_datasets': ['cc-by', 'cc-by-nc'], 'output_datasets': ['cc']}}\n",
    "    >>> add_transformer(input_datasets=['cc-by', 'cc-by-nc'], output_datasets='cc', write_to_catalog=False)\n",
    "    {'_cc': {'input_datasets': ['cc-by', 'cc-by-nc'], 'output_datasets': ['cc']}}\n",
    "    \n",
    "    Names can be given explicitly:\n",
    "    \n",
    "    >>> add_transformer(input_datasets=['cc'], output_datasets=['cc_train','cc_test'], write_to_catalog=False)\n",
    "    {'_cc_train_cc_test': {'input_datasets': ['cc'], 'output_datasets': ['cc_train', 'cc_test']}}\n",
    "    >>> add_transformer(input_datasets=['cc'], output_datasets=['cc_train','cc_test'], name='tts', write_to_catalog=False)\n",
    "    {'tts': {'input_datasets': ['cc'], 'output_datasets': ['cc_train', 'cc_test']}}\n",
    "    \n",
    "    \n",
    "    Invalid use cases:\n",
    "    \n",
    "    >>> add_transformer(datasource_name=\"foo\", output_datasets=['bar', 'baz'])\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Edges from data sources must have only one output_dataset.\n",
    "\n",
    "    >>> add_transformer(datasource_name=\"foo\", input_datasets='bar')\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Cannot set both `datasource_name` and `input_datasets`\n",
    "\n",
    "    >>> add_transformer(datasource_opts={'foo':'bar'})\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Must specify `datasource_name` when using `datasource_opts`\n",
    "        \n",
    "    >>> add_transformer(output_datasets=\"foo\")\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Must specify one of from `datasource_name` or `input_datasets`\n",
    "    \n",
    "    >>> add_transformer(input_datasets=\"foo\")\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Must specify `output_dataset` (or use `suppress_output`)\n",
    "    \"\"\"\n",
    "    input_datasets = normalize_to_list(input_datasets)\n",
    "    output_datasets = normalize_to_list(output_datasets)\n",
    "\n",
    "    if datasource_name is not None:\n",
    "        if input_datasets:\n",
    "            raise Exception('Cannot set both `datasource_name` and `input_datasets`')\n",
    "        if output_datasets is not None and len(output_datasets) > 1:\n",
    "            raise Exception(\"Edges from data sources must have only one output_dataset.\")\n",
    "    if datasource_name is None and datasource_opts is not None:\n",
    "        raise Exception('Must specify `datasource_name` when using `datasource_opts`')\n",
    "\n",
    "    if write_to_catalog:\n",
    "        ds_dag, ds_dag_fq = get_transformer_dag(dag_path=dag_path,\n",
    "                                                dag_file=dag_file,\n",
    "                                                include_filename=True)\n",
    "    transformer = {}\n",
    "    if datasource_name:\n",
    "        transformer['datasource_name'] = datasource_name\n",
    "        if not output_datasets and not suppress_output:\n",
    "            output_datasets = [datasource_name]\n",
    "    elif input_datasets:\n",
    "        transformer['input_datasets'] = input_datasets\n",
    "    else:\n",
    "        raise Exception(\"Must specify one of from `datasource_name` or `input_datasets`\")\n",
    "\n",
    "    if datasource_opts:\n",
    "        transformer['datasource_opts'] = datasource_opts\n",
    "\n",
    "    if transformations:\n",
    "        transformer['transformations'] = transformations\n",
    "\n",
    "    if not suppress_output:\n",
    "        if not output_datasets:\n",
    "            raise Exception(\"Must specify `output_dataset` (or use `suppress_output`)\")\n",
    "        else:\n",
    "            transformer['output_datasets'] = output_datasets\n",
    "\n",
    "    if name is None:\n",
    "        name = f\"_{'_'.join([ids for ids in output_datasets])}\"\n",
    "        \n",
    "    if write_to_catalog:\n",
    "        ds_dag[name] = transformer\n",
    "        save_json(ds_dag_fq, ds_dag)\n",
    "    return {name:transformer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in ['cc-by', 'cc-by-nc', 'other']:\n",
    "    add_transformer(datasource_name=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_cc': {'input_datasets': ['cc-by', 'cc-by-nc'],\n",
       "  'transformations': [('merge', {})],\n",
       "  'output_datasets': ['cc']}}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets=['cc-by', 'cc-by-nc'], output_datasets=['cc'], transformations=[('merge', {})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_cc': {'input_datasets': ['cc'],\n",
       "  'transformations': [('process', {})],\n",
       "  'output_datasets': ['p_cc']}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets='cc', output_datasets='p_cc', transformations=[('process',{})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_cc_train_p_cc_test': {'input_datasets': ['p_cc'],\n",
       "  'transformations': [('ttsplit', {})],\n",
       "  'output_datasets': ['p_cc_train', 'p_cc_test']}}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets='p_cc', output_datasets=['p_cc_train', 'p_cc_test'], transformations=[('ttsplit',{})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_all': {'input_datasets': ['cc', 'other'],\n",
       "  'transformations': [('merge', {})],\n",
       "  'output_datasets': ['p_all']}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets=['cc', 'other'], output_datasets=\"p_all\", transformations=[('merge', {})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_all_train_p_all_test': {'input_datasets': ['p_all'],\n",
       "  'transformations': [('ttsplit', {})],\n",
       "  'output_datasets': ['p_all_train', 'p_all_test']}}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets='p_all', output_datasets=['p_all_train', 'p_all_test'], transformations=[('ttsplit',{})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = get_transformer_dag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n",
      "cc-by\n",
      "cc-by-nc\n",
      "other\n",
      "p_all\n",
      "p_all_train\n",
      "p_all_test\n",
      "p_cc\n",
      "p_cc_train\n",
      "p_cc_test\n"
     ]
    }
   ],
   "source": [
    "for he_name, he in dag.items():\n",
    "    for out in he['output_datasets']:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDAG:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._dag = get_transformer_dag(**kwargs)\n",
    "        self.out_degrees = Counter()\n",
    "        self.in_degrees = Counter()\n",
    "        for n in self.nodes:\n",
    "            self.in_degrees[n] = 0\n",
    "            self.out_degrees[n] = 0\n",
    "        for he_name, he in self._dag.items():\n",
    "            for node in he.get('input_datasets', []):\n",
    "                self.out_degrees[node] += 1\n",
    "            if he.get('datasource_name', False):\n",
    "                self.in_degrees[node] = 0\n",
    "            else:\n",
    "                for node in he['output_datasets']:\n",
    "                    self.in_degrees[node] += 1\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def nodes(self):\n",
    "        ret = set()\n",
    "        for he_name, he in self._dag.items():\n",
    "            for node in he['output_datasets']:\n",
    "                ret.add(node)\n",
    "        return ret\n",
    "    @property\n",
    "    def sources(self):\n",
    "        ret = set()\n",
    "        for he_name, he in self._dag.items():\n",
    "            if he.get('datasource_name', []):\n",
    "                for node in he['output_datasets']:\n",
    "                    ret.add(node)\n",
    "        return ret\n",
    "    \n",
    "    @property\n",
    "    def sinks(self):\n",
    "        n = self.nodes\n",
    "        return n.difference(set(self.in_degrees.keys()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = HDAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cc',\n",
       " 'cc-by',\n",
       " 'cc-by-nc',\n",
       " 'other',\n",
       " 'p_all',\n",
       " 'p_all_test',\n",
       " 'p_all_train',\n",
       " 'p_cc',\n",
       " 'p_cc_test',\n",
       " 'p_cc_train'}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'other': 1,\n",
       "         'p_cc': 1,\n",
       "         'cc-by-nc': 1,\n",
       "         'p_all_train': 0,\n",
       "         'p_all_test': 0,\n",
       "         'p_cc_test': 0,\n",
       "         'cc-by': 1,\n",
       "         'cc': 2,\n",
       "         'p_cc_train': 0,\n",
       "         'p_all': 1})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.in_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'other': 0,\n",
       "         'p_cc': 1,\n",
       "         'cc-by-nc': 0,\n",
       "         'p_all_train': 1,\n",
       "         'p_all_test': 1,\n",
       "         'p_cc_test': 1,\n",
       "         'cc-by': 0,\n",
       "         'cc': 0,\n",
       "         'p_cc_train': 1,\n",
       "         'p_all': 1})"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.out_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cc-by', 'cc-by-nc', 'other'}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"_cc\": {\r\n",
      "    \"input_datasets\": [\r\n",
      "      \"cc-by\",\r\n",
      "      \"cc-by-nc\"\r\n",
      "    ],\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"cc\"\r\n",
      "    ],\r\n",
      "    \"transformations\": [\r\n",
      "      [\r\n",
      "        \"merge\",\r\n",
      "        {}\r\n",
      "      ]\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_cc-by\": {\r\n",
      "    \"datasource_name\": \"cc-by\",\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"cc-by\"\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_cc-by-nc\": {\r\n",
      "    \"datasource_name\": \"cc-by-nc\",\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"cc-by-nc\"\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_other\": {\r\n",
      "    \"datasource_name\": \"other\",\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"other\"\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_p_all\": {\r\n",
      "    \"input_datasets\": [\r\n",
      "      \"cc\",\r\n",
      "      \"other\"\r\n",
      "    ],\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"p_all\"\r\n",
      "    ],\r\n",
      "    \"transformations\": [\r\n",
      "      [\r\n",
      "        \"merge\",\r\n",
      "        {}\r\n",
      "      ]\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_p_all_train_p_all_test\": {\r\n",
      "    \"input_datasets\": [\r\n",
      "      \"p_all\"\r\n",
      "    ],\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"p_all_train\",\r\n",
      "      \"p_all_test\"\r\n",
      "    ],\r\n",
      "    \"transformations\": [\r\n",
      "      [\r\n",
      "        \"ttsplit\",\r\n",
      "        {}\r\n",
      "      ]\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_p_cc\": {\r\n",
      "    \"input_datasets\": [\r\n",
      "      \"cc\"\r\n",
      "    ],\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"p_cc\"\r\n",
      "    ],\r\n",
      "    \"transformations\": [\r\n",
      "      [\r\n",
      "        \"process\",\r\n",
      "        {}\r\n",
      "      ]\r\n",
      "    ]\r\n",
      "  },\r\n",
      "  \"_p_cc_train_p_cc_test\": {\r\n",
      "    \"input_datasets\": [\r\n",
      "      \"p_cc\"\r\n",
      "    ],\r\n",
      "    \"output_datasets\": [\r\n",
      "      \"p_cc_train\",\r\n",
      "      \"p_cc_test\"\r\n",
      "    ],\r\n",
      "    \"transformations\": [\r\n",
      "      [\r\n",
      "        \"ttsplit\",\r\n",
      "        {}\r\n",
      "      ]\r\n",
      "    ]\r\n",
      "  }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat $cp/transformer_dag.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reproallthethings] *",
   "language": "python",
   "name": "conda-env-reproallthethings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
