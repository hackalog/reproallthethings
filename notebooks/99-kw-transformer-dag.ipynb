{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import paths\n",
    "from src.utils import save_json, load_json\n",
    "from src.log import logger\n",
    "from src.data.datasets import cached_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp  = paths['catalog_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset DAG Spec\n",
    "\n",
    "A transformer function takes in `input_datasets` and produces `output_datasets`.\n",
    "Edges can be thought of as directed, indicating a dependency. e.g. `output_datasets` depend on `input_datasets`\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"hyperedge_1\": {\n",
    "        \"input_datasets\":[],\n",
    "        \"output_datasets\":[],\n",
    "        transformations: [\n",
    "            (function_1, kwargs_dict_1 ),\n",
    "            (function_2, kwargs_dict_2 ),\n",
    "            ...\n",
    "        ],\n",
    "        \"suppress_output\": False,  # defaults to True\n",
    "    },\n",
    "    \"sink_edge\": {\n",
    "        \"datasource_name\": \"ds_name\",\n",
    "        \"datasource_opts\": {},\n",
    "        \"output_dataset: \"ds_name\",\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "* Are Sink edges special?\n",
    "  Right now they define a 1-1 map between a datasource and a dataset. (i.e. these are not hyperedges).\n",
    "\n",
    "* How to the list of transformers work with many in many out. Take/emit a dict of all of them?\n",
    "\n",
    "* How to implement traversal. (Apply_transforms should take a dataset name - optionally, and give me an ordered list of what to run)\n",
    "\n",
    "* add force flag to add_transformer, in case you are overwriting a key\n",
    "\n",
    "* should it be input_dataset or input_datasets? The most common use case is probably the former\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_list(str_or_iterable):\n",
    "    \"\"\"Convert strings to lists. Pass lists (or None) unchanged.\n",
    "    \"\"\"\n",
    "    if isinstance(str_or_iterable, str):\n",
    "        return [str_or_iterable]\n",
    "    if str_or_iterable is None:\n",
    "        return []\n",
    "    return str_or_iterable\n",
    "\n",
    "def get_transformers(\n",
    "        transformer_path=None,\n",
    "        transformer_file=None,\n",
    "        include_filename=False,\n",
    "    ):\n",
    "    \"\"\"Get the dictionary of transformers (edges in the transformer graph)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    If include_filename is True:\n",
    "        A tuple: (transformer_dict, transformer_file_fq)\n",
    "    else:\n",
    "        transformer_dict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_filename: boolean\n",
    "        if True, returns a tuple: (list, filename)\n",
    "    transformer_path: path. (default: paths['catalog_dir'])\n",
    "        Location of `transformer_file`\n",
    "    transformer_file: str, default 'transformers.json'\n",
    "        Name of json file that contains the transformer pipeline\n",
    "    \"\"\"\n",
    "    if transformer_path is None:\n",
    "        transformer_path = paths['catalog_path']\n",
    "    else:\n",
    "        transformer_path = pathlib.Path(dag_path)\n",
    "    if transformer_file is None:\n",
    "        transformer_file = 'transformers.json'\n",
    "\n",
    "    transformer_file_fq = transformer_path / transformer_file\n",
    "    try:\n",
    "        transformer_dict = load_json(transformer_file_fq)\n",
    "    except FileNotFoundError:\n",
    "        transformer_dict = {}\n",
    "        \n",
    "    if not isinstance(transformer_dict, dict):\n",
    "        raise Exception(f\"Obsolete file format: {transformer_file} must contain a dict.\")\n",
    "\n",
    "    if include_filename:\n",
    "        return transformer_dict, transformer_file_fq\n",
    "    return transformer_dict\n",
    "\n",
    "\n",
    "def add_transformer(\n",
    "    name=None,\n",
    "    datasource_name=None,\n",
    "    datasource_opts=None,\n",
    "    input_datasets=None,\n",
    "    suppress_output=False,\n",
    "    output_datasets=None,\n",
    "    transformations=None,\n",
    "    dag_path=None,\n",
    "    edge_file=None,\n",
    "    node_file=None,\n",
    "    write_to_catalog=True,\n",
    "    ):\n",
    "    \"\"\"Create and add a dataset transformation pipeline to the workflow.\n",
    "\n",
    "    Transformer pipelines apply a sequence of transformer functions to a Dataset (or DataSource),\n",
    "    to produce new Dataset objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "        Name for this transformer instance (must be unique).\n",
    "        By default, one will be created from the input and output dataset names; e.g.\n",
    "        _input_ds1_input_ds2_to_output_ds1\n",
    "    input_datasets: string or iterable\n",
    "        Upstream data dependencies. These must be present\n",
    "    output_datasets: string or Iterable\n",
    "        These datasets will be generated\n",
    "    datasource_name: string\n",
    "        Name of a DataSource to use to generate the output\n",
    "        Setting this option will create a source node in the dataset flow graph\n",
    "        (or a sink node in the data dependency graph).\n",
    "        Transformers of this type must specify at most one entry in `output_datasets`\n",
    "    datasource_opts: dict\n",
    "        Options to use when generating a Dataset from this DataSource\n",
    "    suppress_output: boolean\n",
    "        If True, the terminal dataset object is not written to disk.\n",
    "        This is useful when one of the intervening tranformers handles the writing; e.g. train/test split.\n",
    "    transformations: list of tuples\n",
    "        Squence of transformer functions to apply. tuples consist of:\n",
    "        (transformer_name, transformer_opts)\n",
    "    dag_path: path. (default: paths['catalog_path'])\n",
    "        Location of `dag_file`\n",
    "    edge_file: string, default 'transformers.json'\n",
    "        Name of json file that contains the transformer pipeline\n",
    "    node_file: string, default 'datasets.json'\n",
    "        Name of json file that contains the dataset metadata\n",
    "    write_to_catalog: Boolean, Default True\n",
    "        If False, don't actually write this entry to the catalog.\n",
    "    Examples\n",
    "    --------\n",
    "    \n",
    "    If you only have one input or output, it may be specified simply as a string;\n",
    "    i.e. these are identical\n",
    "    >>> add_transformer(input_datasets='other', output_datasets='p_other', write_to_catalog=False)\n",
    "    {'_p_other': {'input_datasets': ['other'], 'output_datasets': ['p_other']}}\n",
    "    >>> add_transformer(input_datasets=['other'], output_datasets='p_other', write_to_catalog=False)\n",
    "    {'_p_other': {'input_datasets': ['other'], 'output_datasets': ['p_other']}}\n",
    "    \n",
    "    >>> add_transformer(input_datasets=['cc-by', 'cc-by-nc'], output_datasets='cc', write_to_catalog=False)\n",
    "    {'_cc': {'input_datasets': ['cc-by', 'cc-by-nc'], 'output_datasets': ['cc']}}\n",
    "    >>> add_transformer(input_datasets=['cc-by', 'cc-by-nc'], output_datasets='cc', write_to_catalog=False)\n",
    "    {'_cc': {'input_datasets': ['cc-by', 'cc-by-nc'], 'output_datasets': ['cc']}}\n",
    "    \n",
    "    Names can be given explicitly:\n",
    "    \n",
    "    >>> add_transformer(input_datasets=['cc'], output_datasets=['cc_train','cc_test'], write_to_catalog=False)\n",
    "    {'_cc_train_cc_test': {'input_datasets': ['cc'], 'output_datasets': ['cc_train', 'cc_test']}}\n",
    "    >>> add_transformer(input_datasets=['cc'], output_datasets=['cc_train','cc_test'], name='tts', write_to_catalog=False)\n",
    "    {'tts': {'input_datasets': ['cc'], 'output_datasets': ['cc_train', 'cc_test']}}\n",
    "    \n",
    "    \n",
    "    Invalid use cases:\n",
    "    \n",
    "    >>> add_transformer(datasource_name=\"foo\", output_datasets=['bar', 'baz'])\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Edges from data sources must have only one output_dataset.\n",
    "\n",
    "    >>> add_transformer(datasource_name=\"foo\", input_datasets='bar')\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Cannot set both `datasource_name` and `input_datasets`\n",
    "\n",
    "    >>> add_transformer(datasource_opts={'foo':'bar'})\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Must specify `datasource_name` when using `datasource_opts`\n",
    "        \n",
    "    >>> add_transformer(output_datasets=\"foo\")\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Must specify one of from `datasource_name` or `input_datasets`\n",
    "    \n",
    "    >>> add_transformer(input_datasets=\"foo\")\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    Exception: Must specify `output_dataset` (or use `suppress_output`)\n",
    "    \"\"\"\n",
    "    input_datasets = normalize_to_list(input_datasets)\n",
    "    output_datasets = normalize_to_list(output_datasets)\n",
    "\n",
    "    if datasource_name is not None:\n",
    "        if input_datasets:\n",
    "            raise Exception('Cannot set both `datasource_name` and `input_datasets`')\n",
    "        if output_datasets is not None and len(output_datasets) > 1:\n",
    "            raise Exception(\"Edges from data sources must have only one output_dataset.\")\n",
    "    if datasource_name is None and datasource_opts is not None:\n",
    "        raise Exception('Must specify `datasource_name` when using `datasource_opts`')\n",
    "\n",
    "    if write_to_catalog:\n",
    "        ds_dag, ds_dag_fq = get_transformers(transformer_path=dag_path,\n",
    "                                             transformer_file=edge_file,\n",
    "                                             include_filename=True)\n",
    "    transformer = {}\n",
    "    if datasource_name:\n",
    "        transformer['datasource_name'] = datasource_name\n",
    "        if not output_datasets and not suppress_output:\n",
    "            output_datasets = [datasource_name]\n",
    "    elif input_datasets:\n",
    "        transformer['input_datasets'] = input_datasets\n",
    "    else:\n",
    "        raise Exception(\"Must specify one of from `datasource_name` or `input_datasets`\")\n",
    "\n",
    "    if datasource_opts:\n",
    "        transformer['datasource_opts'] = datasource_opts\n",
    "\n",
    "    if transformations:\n",
    "        transformer['transformations'] = transformations\n",
    "\n",
    "    if not suppress_output:\n",
    "        if not output_datasets:\n",
    "            raise Exception(\"Must specify `output_dataset` (or use `suppress_output`)\")\n",
    "        else:\n",
    "            transformer['output_datasets'] = output_datasets\n",
    "\n",
    "    if name is None:\n",
    "        name = f\"_{'_'.join([ids for ids in output_datasets])}\"\n",
    "        \n",
    "    if write_to_catalog:\n",
    "        ds_dag[name] = transformer\n",
    "        save_json(ds_dag_fq, ds_dag)\n",
    "    return {name:transformer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in ['cc-by', 'cc-by-nc', 'other']:\n",
    "    add_transformer(datasource_name=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_cc': {'input_datasets': ['cc-by', 'cc-by-nc'],\n",
       "  'transformations': [('merge', {})],\n",
       "  'output_datasets': ['cc']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets=['cc-by', 'cc-by-nc'],\n",
    "                output_datasets=['cc'],\n",
    "                transformations=[('merge', {})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_cc': {'input_datasets': ['cc'],\n",
       "  'transformations': [('process_1', {'opts_1': None}),\n",
       "   ('process_2', {'opts_2': None})],\n",
       "  'output_datasets': ['p_cc']}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets='cc',\n",
    "                output_datasets='p_cc',\n",
    "                transformations=[('process_1',{'opts_1':None}), ('process_2', {'opts_2':None})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_cc_train_p_cc_test': {'input_datasets': ['p_cc'],\n",
       "  'transformations': [('ttsplit', {'test_percent': 15})],\n",
       "  'output_datasets': ['p_cc_train', 'p_cc_test']}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets='p_cc',\n",
    "                output_datasets=['p_cc_train', 'p_cc_test'],\n",
    "                transformations=[('ttsplit',{'test_percent':15})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_all': {'input_datasets': ['cc', 'other'],\n",
       "  'transformations': [('join', {'right': 'cc', 'left': 'other'})],\n",
       "  'output_datasets': ['p_all']}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets=['cc', 'other'],\n",
    "                output_datasets=\"p_all\",\n",
    "                transformations=[('join', {'right':'cc', 'left':'other'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_p_all_train_p_all_test': {'input_datasets': ['p_all'],\n",
       "  'transformations': [('ttsplit', {'test_percent': 10})],\n",
       "  'output_datasets': ['p_all_train', 'p_all_test']}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_transformer(input_datasets='p_all',\n",
    "                output_datasets=['p_all_train', 'p_all_test'],\n",
    "                transformations=[('ttsplit', {'test_percent':10})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = get_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n",
      "cc-by\n",
      "cc-by-nc\n",
      "other\n",
      "p_all\n",
      "p_all_train\n",
      "p_all_test\n",
      "p_cc\n",
      "p_cc_train\n",
      "p_cc_test\n"
     ]
    }
   ],
   "source": [
    "for he_name, he in dag.items():\n",
    "    for out in he['output_datasets']:\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerGraph:\n",
    "    \"\"\"Transformer side of the bipartite Data Dependency Graph\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialization parmeters are the same as for `get_transformers`\"\"\"\n",
    "        self._dag = get_transformers(**kwargs)\n",
    "        self.edges_out = {}\n",
    "        self.edges_in = {}\n",
    "        for n in self.nodes:\n",
    "            self.edges_in[n] = 0\n",
    "            self.edges_out[n] = 0\n",
    "        for he_name, he in self._dag.items():\n",
    "            for node in he['output_datasets']:\n",
    "                self.edges_in[node] += 1\n",
    "            for node in he.get('input_datasets', []):\n",
    "                self.edges_out[node] += 1\n",
    "            else:\n",
    "                if self.is_source(he_name):\n",
    "                    self.edges_in[node] = 0               \n",
    "\n",
    "    @property\n",
    "    def nodes(self):\n",
    "        ret = set()\n",
    "        for he in self._dag.values():\n",
    "            for node in he['output_datasets']:\n",
    "                ret.add(node)\n",
    "        return ret\n",
    "\n",
    "    @property\n",
    "    def edges(self):\n",
    "        return self._dag\n",
    "    \n",
    "    @property\n",
    "    def edge(self):\n",
    "        return self._dag\n",
    "    \n",
    "    @property\n",
    "    def sources(self):\n",
    "        return [n for (n, count) in self.edges_in.items() if count < 1]\n",
    "   \n",
    "    @property\n",
    "    def sinks(self):\n",
    "        return [n for (n, count) in self.edges_out.items() if count < 1]\n",
    "    \n",
    "    def find_child(self, node):\n",
    "        \"\"\"Find its parents, siblings and the edge that produced a given child node.\n",
    "        Parameters\n",
    "        ----------\n",
    "        node: String\n",
    "            name of an output node\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (parents, edge, siblings) where\n",
    "        \n",
    "        parents: Set(str)\n",
    "            parents needed to generate this child node\n",
    "        edge: str\n",
    "            name of the edge that generated this node\n",
    "        siblings: Set(str)\n",
    "            set of all the output nodes generated by this edge\n",
    "        \n",
    "        \"\"\"\n",
    "        for hename, he in self._dag.items():\n",
    "            if node in he['output_datasets']:\n",
    "                return set(he.get('input_datasets', [])), hename, set(he['output_datasets'])\n",
    "    \n",
    "    def traverse(self, start, kind=\"breadth-first\"):\n",
    "        \"\"\"Given a start node, trace the graph back to source nodes\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        start: string\n",
    "            Name of start node. Dendencies will be traced form this node back to sources\n",
    "            \n",
    "        kind: {'depth-first', 'breadth-first'}. Default 'breadth-first'\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (nodes, edges) where:\n",
    "        nodes: List(str)\n",
    "            list of node names traversed in the dependency graph\n",
    "        edges: List(str)\n",
    "            list of edge names traversed in the dependcy graph\n",
    "        \"\"\"\n",
    "        if kind == 'breadth-first':\n",
    "            pop_loc = 0\n",
    "        elif kind == 'depth-first':\n",
    "            pop_loc = -1\n",
    "        else:\n",
    "            raise Exception(f\"Unknown kind: {kind}\")\n",
    "        visited = []\n",
    "        edges = []\n",
    "        queue = [start]\n",
    "        while queue:\n",
    "            vertex = queue.pop(pop_loc)\n",
    "            if vertex not in visited:\n",
    "                visited += [vertex]\n",
    "                parents, edge, children = self.find_child(vertex)\n",
    "                queue.extend(parents - set(visited))\n",
    "                edges += [edge]\n",
    "        return list(reversed(visited)), list(reversed(edges))\n",
    "                \n",
    "    def is_source(self, edge):\n",
    "        \"\"\"Is this a source?\n",
    "\n",
    "        Source edges terminate at a DataSource, and are identified\n",
    "        by the presence of a `datasource_name` field\n",
    "        \"\"\"\n",
    "        he = self._dag[edge]\n",
    "        if not he.get('datasource_name', False) and not he.get('input_datasets', False):\n",
    "            raise Exception(\"Invalid Edge: missing both `datasource_name` and `input_datasets`\")\n",
    "        return he.get('datasource_name', False)\n",
    "\n",
    "    def traverse2(self, node, kind=\"breadth-first\", force=False):\n",
    "        \"\"\"Find the path needed to regenerate the given node\n",
    "        \n",
    "        Traverse the graph as far as necessary to regenerate `node`.\n",
    "        \n",
    "        \n",
    "        This will stop at the first upstream node whose parents are fully satisfied,\n",
    "        or all the way to source nodes, depending on the setting of `force`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        start: string\n",
    "            Name of start node. Dendencies will be traced form this node back to sources\n",
    "            \n",
    "        kind: {'depth-first', 'breadth-first'}. Default 'breadth-first'\n",
    "        force: Boolean\n",
    "            if True, stop when all upstream dependencies are satisfied\n",
    "            if False, always traverse all the way to source nodes.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        (nodes, edges)\n",
    "        where:\n",
    "            nodes: List(str)\n",
    "                list of node names traversed in the dependency graph\n",
    "            edges: List(str)\n",
    "                list of edge names traversed in the dependcy graph\n",
    "        \"\"\"\n",
    "        if kind == 'breadth-first':\n",
    "            pop_loc = 0\n",
    "        elif kind == 'depth-first':\n",
    "            pop_loc = -1\n",
    "        else:\n",
    "            raise Exception(f\"Unknown kind: {kind}\")\n",
    "        visited = []\n",
    "        edges = []\n",
    "        queue = [node]\n",
    "        while queue:\n",
    "            vertex = queue.pop(pop_loc)\n",
    "            if vertex not in visited:\n",
    "                visited += [vertex]\n",
    "                parents, edge, children = self.find_child(vertex)\n",
    "                if not self.fully_satisfied(edge) or force:\n",
    "                    logger.debug(f\"Parent dependencies {parents} not satisfied for edge={edge}.\")\n",
    "                    queue.extend(parents - set(visited))\n",
    "                edges += [edge]\n",
    "        return list(reversed(visited)), list(reversed(edges))\n",
    "\n",
    "    def fully_satisfied(self, edge, update_meta=False):\n",
    "        \"\"\"Determine whether all dependencies of the given edge (transformer) are satisfied\n",
    "\n",
    "        Satisfied here means all input datasets are present on disk with valid hashes.\n",
    "        Sources are always considered satisfied\n",
    "        \n",
    "        update_meta: Boolean\n",
    "            if True, cached dataset metadata (including hashes) will be overwritten with whatever\n",
    "            is currently on disk\n",
    "            if False\n",
    "        \"\"\"\n",
    "        if self.is_source(edge):\n",
    "            return True\n",
    "        \n",
    "        input_datasets = self._dag[edge].get('input_datasets', [])\n",
    "        for ds_name in input_datasets:\n",
    "            ds_meta = Dataset.load(ds_name, metadata_only=True, errors=False)\n",
    "            if not ds_meta:  # does not exist\n",
    "                logger.debug(f\"No dataset on-disk for dataset={ds_name}\")\n",
    "                break\n",
    "            if self._nodes[ds_name]['hashes'] != ds_meta['hashes']:\n",
    "                if update_hashes:\n",
    "                    logger.debug(f\"Updating hashes for dataset={ds_name}: {ds_meta['hashes']}\")\n",
    "                    self._nodes[ds_name] = ds_meta\n",
    "                else: # invalid hashes\n",
    "                    logger.warning(f\"Hashes invalid for on-disk dataset={ds_name}: \"\n",
    "                                   f\"{self._nodes[ds_name]['hashes']} != {ds_meta['hashes']}\")\n",
    "                    break\n",
    "        else:  # (no break) all requirements satisfied\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.workflow import available_datasets\n",
    "from src.data.datasets import cached_datasets\n",
    "from src.data.datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../catalog/datasets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 15:32:17,211 - datasets - WARNING - Dataset catalog 'datasets.json' does not exist. Writing new dataset catalog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beer_review_all', 'foo'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 15:32:17,285 - datasets - DEBUG - Updating dataset catalog with 'foo' metadata\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset('foo')\n",
    "ds.update_catalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beer_review_all', 'foo'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 15:32:17,386 - datasets - DEBUG - Wrote Dataset Metadata: foo.metadata\n",
      "2020-04-07 15:32:17,388 - datasets - DEBUG - Updating dataset catalog with 'foo' metadata\n",
      "2020-04-07 15:32:17,389 - datasets - DEBUG - Wrote Dataset: foo.dataset\n"
     ]
    }
   ],
   "source": [
    "ds.dump(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"foo\": {\r\n",
      "    \"dataset_name\": \"foo\",\r\n",
      "    \"hashes\": {\r\n",
      "      \"data\": [\r\n",
      "        \"sha1\",\r\n",
      "        \"38f65f3b11da4851aaaccc19b1f0cf4d3806f83b\"\r\n",
      "      ],\r\n",
      "      \"target\": [\r\n",
      "        \"sha1\",\r\n",
      "        \"38f65f3b11da4851aaaccc19b1f0cf4d3806f83b\"\r\n",
      "      ]\r\n",
      "    }\r\n",
      "  }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat ../catalog/datasets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hashes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1c29c1740e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetadata_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/devel/git/timc/reproallthethings/src/data/datasets.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, dataset_name, data_path, metadata_only, errors, catalog_path, dataset_cache, check_hashes)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_hashes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdataset_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hash mismatch for {dataset_name}: {meta['hashes']} != cache: {dataset_cache['hashes']}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hashes'"
     ]
    }
   ],
   "source": [
    "Dataset.load('foo',metadata_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/kjell/Documents/devel/git/timc/reproallthethings/src/data/datasets.py\u001b[0m(329)\u001b[0;36mload\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    327 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mcheck_hashes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    328 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mdataset_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 329 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hash mismatch for {dataset_name}: {meta['hashes']} != cache: {dataset_cache['hashes']}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    330 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mmetadata_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    331 \u001b[0;31m            \u001b[0;32mreturn\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> meta\n",
      "{'dataset_name': 'foo', 'hashes': {'data': ('sha1', '38f65f3b11da4851aaaccc19b1f0cf4d3806f83b'), 'target': ('sha1', '38f65f3b11da4851aaaccc19b1f0cf4d3806f83b')}}\n",
      "ipdb> dataset_cache\n",
      "{'foo': {'dataset_name': 'foo', 'hashes': {'data': ['sha1', '38f65f3b11da4851aaaccc19b1f0cf4d3806f83b'], 'target': ['sha1', '38f65f3b11da4851aaaccc19b1f0cf4d3806f83b']}}}\n",
      "ipdb> dataset_cache[dataset_name]\n",
      "{'dataset_name': 'foo', 'hashes': {'data': ['sha1', '38f65f3b11da4851aaaccc19b1f0cf4d3806f83b'], 'target': ['sha1', '38f65f3b11da4851aaaccc19b1f0cf4d3806f83b']}}\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = TransformerGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.edges_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.edges_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.sinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.traverse('p_all_train', kind=\"depth-first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.traverse('p_all_train', kind=\"breadth-first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.traverse2('p_all_train', kind=\"breadth-first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_metadata(node):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kwdict_to_list(kw):\n",
    "    return [f\"{k}={v}\" for k,v in kw.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_on_disk(dataset_path=None, keys_only=True):\n",
    "    \"\"\"Get a list of available datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_path: path\n",
    "        location of saved dataset files\n",
    "    \"\"\"\n",
    "    if dataset_path is None:\n",
    "        dataset_path = paths['processed_data_path']\n",
    "    else:\n",
    "        dataset_path = pathlib.Path(dataset_path)\n",
    "\n",
    "    ds_dict = {}\n",
    "    for dsfile in dataset_path.glob(\"*.metadata\"):\n",
    "        ds_stem = str(dsfile.stem)\n",
    "        ds_meta = Dataset.load(ds_stem, data_path=dataset_path, metadata_only=True)\n",
    "        ds_dict[ds_stem] = ds_meta\n",
    "\n",
    "    if keys_only:\n",
    "        return list(ds_dict.keys())\n",
    "    return ds_dict\n",
    "dod = datasets_on_disk(); dod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformers(self, ds_name, force=False, update_hashes=False):\n",
    "    \"\"\"Apply all transformers neede to generate `ds_name`\n",
    "\n",
    "    This walks the dataset dependency graph backwards, applying transformers\n",
    "    until it is able to successfully generate `ds_name`.\n",
    "\n",
    "    XXX: WIP. This currently does all the work, all the time. Need an example to work with to try this out.\n",
    "    \n",
    "    ds_name: str\n",
    "        Name of dataset to be generated\n",
    "    force: Boolean\n",
    "        if True, always apply transformer, even if cached copy exists.\n",
    "        This will always walk back to sources.\n",
    "        If False, the process will stop as soon as the dataset can be generated\n",
    "    update_hashes: Boolean\n",
    "        If True, the dataset catalog will be updated with the new hash\n",
    "        if False, process will stop with an error if the hash changes on any step.\n",
    "    \"\"\"\n",
    "    _, edge_list = dd.traverse(ds_name, kind=\"breadth-first\")\n",
    "    \n",
    "    cached_nodes = cached_datasets(keys_only=False)\n",
    "    for e in edge_list:\n",
    "        logger.debug(f\"Checking Transformer: '{e}'\")\n",
    "        he = self.edge[e]\n",
    "        if he.get('datasource_name', False): # Source node?\n",
    "            assert len(he['output_datasets']) == 1, \"Invalid Source Node: {e}. Too many output datasets\"\n",
    "            output_node = he['output_datasets'][0]\n",
    "            logger.debug(f\"Source Node: '{output_node}'\")\n",
    "            if output_node in self.cached_nodes:\n",
    "                logger.error(f\"TODO: Check hashes on {output_node}\")\n",
    "                continue\n",
    "            dsdict = {output_node:get_node_metadata(output_node)}\n",
    "            kwargs = [f\"'{he['datasource_name']}'\"] + kwdict_to_list(he.get('datasource_opts', {}))\n",
    "            kwarg_str = \", \".join(kwargs)\n",
    "            logger.debug(f\"Generating Dataset: {dsdict} <= process_dataset({kwarg_str})\")\n",
    "        else: # intermediate or sink node\n",
    "            dsdict = {k:get_node_metadata(k) for k in he['input_datasets']}\n",
    "            for t, topts in he.get('transformations', ()):\n",
    "                output_dsdict = {k:get_node_metadata(k) for k in he['output_datasets']}\n",
    "                kwarg_str = \", \".join([str(dsdict)] + kwdict_to_list(topts))\n",
    "                logger.debug(f\"{output_dsdict} <= {t}({kwarg_str})\")\n",
    "                dsdict = output_dsdict\n",
    "            # check if generated hashes match the expected hashes\n",
    "            logger.debug(f\"Writing {output_dsdict} to disk.\")\n",
    "            if update_hashes is True:\n",
    "                logger.debug(f\"Updating hashes in dataset dictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transformers(dd, 'p_cc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.cached_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reproallthethings] *",
   "language": "python",
   "name": "conda-env-reproallthethings-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
