from functools import partial

from .. import paths
from ..log import logger
from .transformers import get_transformer_catalog, dataset_from_datasource
from .datasets import get_dataset_catalog, Dataset, check_dataset_hashes
from .utils import deserialize_partial, partial_call_signature

__all__ = [
    'TransformerGraph',
]

def default_transformer(dsdict, **kwargs):
    """Placeholder for transformerdata processing function"""
    transformer_name = kwargs.get('transformer_name', 'unknown-transformer')
    logger.error(f"'{transformer_name}()' function not found. Define it add it to the `user` namespace for correct behavior")
    return dsdict


class TransformerGraph:
    """Transformer side of the bipartite Dataset Dependency Graph

    A "transformer" is a function that:

    * takes in zero or more `Dataset` objects (the `input_datasets`),
    * produces one or more `Dataset` objects (the `output_datasets`).

    Edges in this graph are directed, indicating the direction the direction of data dependencies.
    e.g. `output_datasets` depend on `input_datasets`.


    While the functions themselves are stores in the source module (default `src/user/transformers.py`),
    metadata describing these functions and which `Dataset` objects are generated are
    serialzed to `paths['catalog_path']/transformers.json`.

    Properties
    ----------
    nodes: set of dataset nodes (nodes in the hypergraph)
    edges: set of transformer nodes (edges in the hypergraph)
    """

    def __init__(self, catalog_path=None, transformer_catalog='transformers.json',  dataset_catalog='datasets.json'):
        """Create the Transformer (Dataset Dependency) Graph

        This can be thought of as a bipartite graph (node sets are datasets and transformers respectively), or a hypergraph,
        (nodes=datasets, edges=transformers) depending on your preference.

        catalog_path:
            Location of catalog files. Default paths['catalog_path']
        transformer_catalog:
            Catalog file. default 'transformers.json'. Relative to catalog_path
        dataset_catalog:
            Default 'transformers.json'. Relative to `catalog_path`
        """
        if catalog_path is None:
            catalog_path = paths['catalog_path']
        else:
            catalog_path = pathlib.Path(catalog_path)

        self.transformers = get_transformer_catalog(catalog_path=catalog_path, catalog_file=transformer_catalog)
        self.datasets = get_dataset_catalog(catalog_path=catalog_path, catalog_file=dataset_catalog)

        self._validate_hypergraph()

        self.edges_out = {}
        self.edges_in = {}
        for n in self.nodes:
            self.edges_in[n] = 0
            self.edges_out[n] = 0
        for he_name, he in self.transformers.items():
            for node in he['output_datasets']:
                self.edges_in[node] += 1
            for node in he.get('input_datasets', []):
                self.edges_out[node] += 1
            else:
                if self.is_source(he_name):
                    self.edges_in[node] = 0

    def _validate_hypergraph(self):
        """Check the basic structure of the hypergraph is valid"""

        valid = True
        for node in self.nodes:
            if node not in self.datasets:
                logger.warning(f"Node '{node}' not found in Dataset catalog.")
                valid = False

        return valid

    @property
    def nodes(self):
        ret = set()
        for he in self.transformers.values():
            for node in he['output_datasets']:
                ret.add(node)
        return ret

    @property
    def edges(self):
        return set(self.transformers)

    @property
    def sources(self):
        return [n for (n, count) in self.edges_in.items() if count < 1]

    @property
    def sinks(self):
        return [n for (n, count) in self.edges_out.items() if count < 1]

    def find_child(self, node):
        """Find its parents, siblings and the edge that produced a given child node.
        Parameters
        ----------
        node: String
            name of an output node

        Returns
        -------
        (parents, edge, siblings) where

        parents: Set(str)
            parents needed to generate this child node
        edge: str
            name of the edge that generated this node
        siblings: Set(str)
            set of all the output nodes generated by this edge

        """
        for hename, he in self.transformers.items():
            if node in he['output_datasets']:
                return set(he.get('input_datasets', [])), hename, set(he['output_datasets'])

    def is_source(self, edge):
        """Is this a source?

        Source edges terminate at a DataSource, and are identified
        by the an empty (or missing) input_datasets field
        """
        return not self.transformers[edge].get('input_datasets', False)

    def traverse(self, node, kind="breadth-first", force=False):
        """Find the path needed to regenerate the given node

        Traverse the graph as far as necessary to regenerate `node`.

        This will stop at the first upstream node whose parents are fully satisfied,
        (i.e. cached on disk, and whose hashes match the datset catalog)
        or all the way to source nodes, depending on the setting of `force`.

        Parameters
        ----------
        start: string
            Name of start node. Dendencies will be traced form this node back to sources

        kind: {'depth-first', 'breadth-first'}. Default 'breadth-first'
        force: Boolean
            if True, stop when all upstream dependencies are satisfied
            if False, always traverse all the way to source nodes.

        Returns
        -------
        (nodes , edges)
        where:
            nodes: List(str)
                list of node names traversed in the dependency graph
            edges: List(str)
                list of edge names traversed in the dependcy graph
        """
        if kind == 'breadth-first':
            pop_loc = 0
        elif kind == 'depth-first':
            pop_loc = -1
        else:
            raise Exception(f"Unknown kind: {kind}")
        visited = []
        edges = []
        queue = [node]
        while queue:
            vertex = queue.pop(pop_loc)
            if vertex not in visited:
                visited += [vertex]
                parents, edge, children = self.find_child(vertex)
                satisfied = self.fully_satisfied(edge)
                if satisfied:
                    if force:
                        logger.debug(f"All dependencies {parents} satisfied for edge: '{edge}' but force=True specified.")
                    else:
                        logger.debug(f"All dependencies {parents} satisfied for edge: '{edge}'")
                else:
                    logger.debug(f"Parent dependencies {parents} not satisfied for edge '{edge}'.")
                if not satisfied or force:
                    queue.extend(parents - set(visited))
                edges += [edge]
        return list(reversed(visited)), list(reversed(edges))

    def generate_outputs(self, edge_name):
        """Generate the outputs for a given edge

        This assumes all dependencies are on-disk and have valid caches.

        """
        if not self.fully_satisfied(edge_name):
            raise Exception(f"Edge '{edge_name}' has unsatisfied dependencies.")
        edge = self.transformers[edge_name]
        dsdict = {}
        logger.debug(f"Processing input datasets for Edge '{edge_name}'")
        for in_ds in edge.get('input_datasets', []):  # sources have no inputs
            logger.debug(f"Loading Input Dataset '{in_ds}'")
            if in_ds not in self.datasets:
                raise Exception(f"Edge '{edge_name}' specifies an input dataset, '{in_ds}' that is not in the dataset catalog")
            ds = Dataset.from_disk(in_ds)
            cached_hashes, catalog_hashes = ds.HASHES, self.datasets[in_ds]['hashes']
            if not check_dataset_hashes(cached_hashes, catalog_hashes):
                raise Exception(f"Cached Dataset '{in_ds}' hashes {cached_hashes} do not match catalog {catalog_hashes}")
            dsdict[in_ds] = ds

        for xform, xform_opts in edge.get('transformations', ()):
            transformer = deserialize_partial({'load_function_name':xform, 'load_function_kwargs': xform_opts}, fail_func=partial(default_transformer, transformer_name=xform))
            _, xform_func_str = partial_call_signature(transformer)
            logger.debug(f"Applying transformer: {xform_func_str}")
            dsdict = transformer(dsdict)


    def fully_satisfied(self, edge):
        """Determine whether all dependencies of the given edge (transformer) are satisfied

        Satisfied here means all input datasets are present (cached) on disk with valid hashes.
        Sources are always considered satisfied
        """
        if self.is_source(edge):
            return True

        input_datasets = self.transformers[edge].get('input_datasets', [])

        for ds_name in input_datasets:
            ds_meta = Dataset.load(ds_name, metadata_only=True, errors=False)
            if not ds_meta:  # does not exist
                logger.debug(f"No cached dataset found for dataset '{ds_name}'.")
                return False
            if ds_name not in self.datasets:
                raise Exception(f"Missing '{ds_name}' in dataset catalog")
            cached_hashes, catalog_hashes = ds_meta['hashes'], self.datasets[ds_name]['hashes']
            if not check_dataset_hashes(cached_hashes, catalog_hashes):
                logger.debug(f"Cached dataset '{ds_name}' hash {cached_hashes} != catalog hash {catalog_hashes}")
                return False
        return True

    def generate(dataset_name):
        pass
